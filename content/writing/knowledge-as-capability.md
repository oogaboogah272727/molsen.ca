---
title: "Knowledge as Capability: An Operational Foundation for Knowledge Management"
date: 2024-11-12
description: "Extending knowledge management to accommodate AI by defining knowledge as capability to produce outcomes—the foundational claim that makes human-AI collaboration coherent."
originalPublication: "Originally published November 2024"
type: foundational
tags: ["Epistemic Rigor"]
---

> *This essay provides the foundational definition on which several other pieces in this collection depend. Cross-references reflect the current state of an evolving framework. The version here is authoritative.*

Knowledge management has served organizations well for decades. The frameworks from Nonaka and Takeuchi, the practices around communities of practice, the distinction between tacit and explicit knowledge—these have helped countless organizations preserve and transfer expertise.

But KM's conceptual foundations were built for a world of human knowers. The field inherited its definition of knowledge from philosophy: justified true belief. That works fine when all your agents are humans with mental states. The framework handles human expertise, human communication, human learning.

What it doesn't handle is language models.

When an AI system produces working code, accurate summaries, or valid analyses, the traditional framework has no place to put that capability. The AI doesn't "believe" anything. It has no mental states to justify. Under the inherited definition, it cannot have knowledge. Yet it demonstrably does knowledge work.

I'm arguing for an extended foundation: **knowledge is the capability to reliably produce outcomes**. This definition preserves everything that worked about traditional KM while creating conceptual room for AI capabilities. It's not a replacement but an expansion. One that makes human-AI collaboration coherent within knowledge management frameworks.

Everything else I've written about Executable Knowledge Architecture and agent-relative tacitness rests on this foundation. Without it, AI capabilities sit awkwardly outside KM's scope. With it, human and AI contributions become commensurable.

---

## The Traditional Framework

Philosophers have debated the nature of knowledge for millennia, and I'm not going to pretend that "justified true belief" is the settled answer. It isn't. Gettier demolished the sufficiency of JTB in 1963, and the field has spent sixty years proposing alternatives: reliabilism, virtue epistemology, tracking theories, knowledge-first approaches. The debates are complex and unresolved.

But what matters for our purposes: all these frameworks share an assumption. They assume agents with mental states. Whether you require justification, reliable belief-forming processes, intellectual virtues, or sensitivity to truth, you're still asking about the epistemic status of a *believer*. The debates are about what conditions beliefs must meet to constitute knowledge.

Let me be direct about what "agents with mental states" means in practice: it means humans. And here we should acknowledge an uncomfortable truth. We don't actually know what mental states are. Philosophy hasn't solved the solipsism problem—I cannot verify that you have experiences, only that you behave as if you do. Neuroscience hasn't solved the hard problem of consciousness. Psychology doesn't have a settled account of what cognition is or how it produces understanding. The confident claim that humans *have* knowledge while AI systems *cannot* assumes we know what knowledge-constituting mental states are and how to detect them. We don't. That knowledge is not established in any discipline.

This matters because some people dismiss AI capabilities definitionally—"it's just pattern matching, it doesn't really understand"—while assuming a clarity about human understanding that doesn't exist. You see arguments like: "A model is just a file. The file contains weights. You can hex dump it and inspect the binary. Therefore it can't be intelligent." But this is backwards. You can also describe neurons firing, neurotransmitters crossing synapses, ion channels opening and closing. Mechanism description doesn't settle the question of capability. "It's just matrix multiplication" has exactly the same logical force as "it's just electrochemistry"—which is to say, none. The fact that we can inspect a process tells us nothing about whether that process constitutes understanding. We can inspect human brains too.

The people making these dismissive arguments are the ones engaged in magical thinking. They're treating "intelligence" and "understanding" as settled concepts with known criteria, then confidently pronouncing that AI fails to meet them. But we don't have those criteria. No discipline does. The confident denial assumes more knowledge about the nature of mind than currently exists.

I'm not claiming AI systems have consciousness or experiences. I'm observing that we lack the theoretical foundations to make confident pronouncements either way. The honest position is epistemic humility about mechanisms combined with pragmatic attention to outcomes.

Knowledge management inherited this assumption, even if it never engaged deeply with the post-Gettier complexities. KM practice treats knowledge as something that can be:
- Extracted (because it's a belief state in someone's head)
- Documented (because it's propositional)
- Verified (because it has truth conditions)
- Transferred (because the same proposition can be believed by another)

These assumptions work reasonably well when all your knowers are humans. But they become limiting when AI enters the picture. Not because epistemologists got the debates wrong. Because the entire framework—JTB and its successors alike—was scoped to agents with mental states.

---

## Where Belief-Based Frameworks Reach Their Limits

Even in human-only contexts, belief-based frameworks have recognized limitations. KM practitioners have worked around these for years, but they're worth naming because they point toward the extension we need.

### Organizational Knowledge Is Distributed

Belief-based epistemology requires a believing agent. When we say "the organization knows how to manufacture semiconductors," who is the believer?

Not the CEO, who may not understand the process. Not any individual engineer, who knows only their part. Not the documentation, which is inert. The organization *does* the manufacturing through coordinated execution across people, systems, processes, and artifacts.

KM has always recognized this. That's why we talk about organizational capabilities, not just individual knowledge. But the underlying definition doesn't quite fit. An operational definition handles distributed capability naturally.

### Practical Knowledge Is Validated by Outcomes

Belief-based frameworks require truth conditions that can be evaluated. But organizational knowledge concerns what will work, not what is timelessly true.

- "This sales approach works with enterprise clients"—true until the market shifts
- "This architecture handles our scale requirements"—true until traffic doubles
- "This hiring process finds good engineers"—true until the talent market changes

The truth of practical knowledge is revealed by outcomes, not by correspondence to independent reality. KM practitioners have always known this implicitly. We assess expertise by track record, not by philosophical verification. An operational definition makes this explicit.

### Expertise Often Exceeds Articulation

The people who can actually *do* the work often cannot fully articulate why their approach works. The expert diagnostician will give you a story about their reasoning, but the story is a reconstruction, not a transcript of their actual cognitive process.

This is Polanyi's insight about tacit knowledge: we know more than we can tell. KM has embraced this distinction. But under belief-based frameworks, tacit knowledge is a puzzle. How can you believe something you can't state? An operational definition dissolves the puzzle: capability can exceed articulation without contradiction.

### Procedural Knowledge Resists Propositions

Belief-based epistemology applies to propositions—statements that can be true or false. But much organizational knowledge is not propositional:

- Knowing *how* to negotiate a difficult contract
- Knowing *when* a project is going off track
- Knowing *who* can actually get things done

These are capabilities, sensitivities, and relationships. The expert machinist "knows" when the cut sounds wrong. Not as a proposition about frequencies, but as a perceptual capability.

KM has always worked with procedural and relational knowledge despite the definitional awkwardness. An operational definition eliminates the awkwardness.

---

## The AI Extension Problem

The limitations above are manageable. KM has worked around them for years. But AI creates a harder problem: agents that produce knowledge work without having mental states at all.

### The Belief Question Doesn't Apply

If we insist on belief-based definitions, we must ask: does an LLM *believe* its outputs? This question has no answer because it's built on a category error. LLMs are not the kind of system that has beliefs. The concept simply doesn't apply.

But the LLM can often *do the task*. If I ask it to write a function that sorts a list, it produces working code. If I ask it to summarize a document, it produces an accurate summary.

Under belief-based definitions, we have no way to describe this. Knowledge requires belief. The AI has no beliefs. Therefore the AI has no knowledge. But it demonstrably does knowledge work. The framework lacks vocabulary for what we're observing.

### Capability Without Mental States

AI systems have no beliefs to be justified, no truth-tracking mental states, no justification chains they construct or evaluate. What they have is the ability to produce outputs that accomplish tasks.

If knowledge management stays with belief-based definitions, then AI systems remain permanently outside its scope. This happens even as they increasingly contribute to organizational capability. That's not a failure of KM. It's a scope limitation that needs addressing.

### Agent-Relative Tacitness Becomes Possible

An insight becomes possible with an extended definition: knowledge that is tacit for one agent may be explicit for another.

For a human doctor: "This patient presentation suggests condition X" may be tacit. She can't fully articulate why.

For an AI trained on millions of cases: The same diagnostic conclusion may be entirely computable from patterns in the training data.

Tacitness is not a property of knowledge itself but of the relationship between knowledge and knower. This only makes sense if knowledge is understood as capability. A capability is tacit when the agent can exercise it but cannot articulate how. That's agent-relative by definition.

Under belief-based frameworks, this insight is hard to express. Under capability definitions, it becomes natural. And it opens new possibilities for human-AI collaboration in knowledge transfer.

---

## The Operational Definition

Here is the alternative:

**An agent has knowledge K with respect to outcome O in context C if and only if the agent can reliably produce O in C.**

Let me break this down:

**Agent**: Any system capable of action—human, AI, organization, human-AI hybrid. The underlying implementation is irrelevant. What matters is whether it can act.

**Outcome**: Observable, verifiable state of the world. Working code. Healed patient. Profitable trade. Valid proof. Not a belief state. A state of affairs.

**Context**: The conditions under which capability is claimed. Knowledge claims are always bounded. "Can diagnose appendicitis in routine presentations" is a different claim than "can diagnose any abdominal condition."

**Reliably**: Not once, not by luck, but consistently across instances. The reliability threshold varies by domain—surgery demands higher reliability than brainstorming—but the requirement for repeatability is universal.

This definition has several features that matter:

**It's operational.** You verify knowledge by observing performance, not by inspecting mental states.

**It's agent-relative.** The same capability may be present in one agent and absent in another, tacit for one and explicit for another.

**It's measurable.** Degrees of knowledge correspond to reliability of outcome production across context instances.

**It handles AI naturally.** If the system can do the task, it has the knowledge. Whether it "understands" is immaterial.

---

## What Changes With an Operational Definition

Adopting the capability definition doesn't invalidate existing KM practice. It clarifies what that practice was already doing implicitly and extends it to new contexts.

### Documentation Becomes One Tool Among Many

Under a capability definition, documentation is valuable when it transfers capability—when someone who reads it can then do the task. This is exactly how good KM practitioners have always evaluated documentation.

The definition makes explicit what was implicit: the goal is capability transfer, not documentation for its own sake. This clarifies when documentation works (procedures that can be followed), when apprenticeship works (skills that must be demonstrated), and when system-building works (capabilities that can be embedded in tools).

### Knowledge Capture Becomes Capability Transfer

When we ask "how do we capture what this expert knows before they retire?" the capability definition reframes the question: "how do we transfer this person's capability to other agents?"

Sometimes the answer is documentation. Sometimes it's pairing and apprenticeship. Sometimes it's building systems that embody the capability. And now, sometimes it's working with AI to articulate what the expert couldn't articulate alone.

The definition doesn't change the goal. It clarifies that the goal was always capability transfer, and opens new methods for achieving it.

### Assessment Focuses on Outcomes

The capability definition aligns with how we actually evaluate expertise: by track record, by demonstrated performance, by whether interventions work.

Credentials and certifications remain useful as *signals* of likely capability. But the definition makes clear they're proxies, not the thing itself. This matches how experienced KM practitioners already think about expertise.

---

## What This Foundation Makes Possible

### Executable Knowledge Architecture

EKA implements [assistant mode](/writing/ai-oracle-vs-assistant/) for professional work: it asks AI to translate expert intent into executable code rather than to provide answers directly. This only makes sense under a capability definition of knowledge.

The expert has knowledge: the capability to recognize correct outcomes, specify intent, and verify alignment. The AI has different knowledge: the capability to produce executable implementations from specifications.

Neither alone constitutes the knowledge needed to solve problems reliably. Together, through a verification loop, they produce validated solutions. The composite system has knowledge that neither component has alone.

Under belief-based definitions, this is incoherent. Who "believes" the solution? Under capability definitions, it's precise. The human-AI system can reliably produce verified solutions.

### Agent-Relative Tacitness

Tacit knowledge has always been a puzzle for KM. If knowledge requires belief, what does it mean for a belief to be "tacit"? How can you believe something you can't state?

Under capability definitions, tacitness is straightforward: knowledge is tacit for agent B when no representational path exists from the knower to B. The capability exists but cannot be transferred through any available encoding and decoding process.

This makes tacitness agent-relative rather than intrinsic. The same knowledge can be tacit between a machinist and a novice, explicit between the machinist and another machinist, and newly explicit between the machinist and an AI system that can decode the machinist's demonstrations.

### Tacit Space Shrinkage

As AI capabilities expand, more knowledge becomes transferable that was previously locked in human expertise. An AI that can learn from video demonstrations can decode knowledge that was previously tacit because the only transfer path was embodied apprenticeship.

This "shrinkage of tacit space" is only coherent if tacitness is about transfer relationships, not intrinsic properties. And that's only coherent if knowledge is capability, not belief.

---

## The Lineage

This isn't a novel position I invented to make AI convenient. It draws on established traditions:

**Pragmatic philosophy** (Peirce, James, Dewey): Knowledge defined by practical consequences and action. Dewey's instrumentalism treats ideas as tools for problem-solving, not representations of fixed truths. "Knowledge as an instrument of action."

**Procedural knowledge** (Ryle, Polanyi): "Knowing how" cannot be reduced to "knowing that." We know more than we can tell. Knowledge is about being able, not about holding propositions.

**Organizational capability** (Senge, Sveiby, Teece): "Knowledge is the capacity for effective action. There is no capacity for effective action in a database." Dynamic capabilities as the firm's ability to produce outcomes in changing conditions.

**Cybernetics** (Ashby, Beer): Knowledge as variety management for effective regulation. A control system must be capable of producing the responses required to manage disturbances. That's capability, not content.

The capability view isn't a convenient fiction. It's the natural framework for anyone focused on what organizations can actually do.

---

## For Practitioners

The capability definition adds questions to your toolkit:

**Alongside "what do we know?"** ask "what can we do?" The former maps your documented knowledge; the latter maps your actual capabilities. Both matter.

**Alongside propositional capture**, use practice, pairing, and system-building. Documentation works for some knowledge; other methods work for others. The capability lens helps you choose.

**Alongside documentation metrics**, track outcomes. Completion rates, success rates, capability demonstrations. The goal is capability transfer; measure whether it's happening.

**Include AI in your knowledge systems.** AI capabilities are organizational capabilities. They can receive knowledge transfer (training, prompting) and contribute to knowledge work. The capability definition gives you vocabulary for this.

**Accept that capability can exceed articulation.** Experts who can do but cannot fully explain still have knowledge worth transferring. Work with them to find transfer paths—including AI-assisted articulation.

---

## The Foundation

This definition—knowledge as capability to produce outcomes—is the foundation on which the rest of my framework rests.

Agent-relative tacitness only makes sense if knowledge is capability (because tacitness becomes about transfer of capability, not about hidden beliefs).

Tacit space shrinkage only makes sense if tacitness is agent-relative (because AI capabilities can open new transfer paths).

Executable Knowledge Architecture only makes sense if human-AI systems can have composite knowledge (because capability can be distributed across agents).

The governance challenges I've written about—knowledge that is explicit-for-systems but opaque-for-humans—only make sense if explicitness is about transfer relationships, not about propositional content.

I could be wrong about this framing. But I don't think it's a rejection of knowledge management. It's an extension that preserves what works and creates room for what's new.

We're in the business of doing things. The definition of knowledge should reflect that.
