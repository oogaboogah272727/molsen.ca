---
title: "Knowledge as Capability: An Operational Foundation for Knowledge Management"
date: 2025-01-27
description: "Why knowledge management must abandon epistemology and define knowledge as the capability to produce outcomes—the foundational claim that makes AI-augmented expertise coherent."
---

Knowledge management has a definitional problem it rarely examines. The field inherited a concept of knowledge from philosophy—justified true belief—that was never designed for organizational practice and actively undermines effective KM work.

This paper argues for a different foundation: **knowledge is the capability to reliably produce outcomes**. No belief states. No truth conditions. No justification chains. Just capability, demonstrable through performance.

This isn't philosophical fence-sitting or pragmatic hand-waving. It's a precise redefinition that makes several things possible that are impossible under the traditional view: coherent treatment of tacit knowledge, meaningful integration of AI capabilities, and practical assessment of what organizations actually have when they have "knowledge."

Everything else I've written about Executable Knowledge Architecture and agent-relative tacitness rests on this foundation. Without it, those frameworks inherit epistemological baggage that makes them incoherent. With it, they become operationally precise.

---

## The Inherited Problem

The philosophical definition of knowledge—justified true belief (JTB)—requires three conditions:

1. **Belief**: An agent holds a mental state affirming a proposition
2. **Truth**: The proposition corresponds to reality
3. **Justification**: The agent has adequate reasons for the belief

This framework asks: what must be true for someone to *know* something rather than merely *believe* it?

Knowledge management adopted this framework without much scrutiny. The result is that KM practice treats knowledge as something that can be:
- Extracted (because it's a belief state in someone's head)
- Documented (because it's propositional)
- Verified (because it has truth conditions)
- Transferred (because the same proposition can be believed by another)

Every one of these assumptions is problematic. And the problems aren't subtle edge cases—they're fundamental mismatches between framework and reality that explain why so many KM initiatives fail.

---

## Why Justified True Belief Fails Organizations

### Organizations Don't Believe

JTB requires a believing agent. When we say "the organization knows how to manufacture semiconductors," who is the believer?

Not the CEO, who may not understand the process. Not any individual engineer, who knows only their part. Not the documentation, which is inert. Not some collective consciousness, which doesn't exist.

The organization *does* the manufacturing. It acts. The "knowledge" exists in coordinated execution across people, systems, processes, and artifacts. There is no propositional belief sitting anywhere.

When knowledge managers try to "capture organizational knowledge," they're looking for a believing agent that doesn't exist.

### Truth Is Unknowable When It Matters

JTB requires truth conditions that can be evaluated. But organizational knowledge concerns what will work, not what is timelessly true.

- "This sales approach works with enterprise clients"—true until the market shifts
- "This architecture handles our scale requirements"—true until traffic doubles
- "This hiring process finds good engineers"—true until the talent market changes

The truth of practical knowledge is revealed by outcomes, not by correspondence to independent reality. You find out if you "knew" the right approach by whether it worked. This means JTB creates a perverse situation: you can only confirm knowledge *after* you no longer need it.

### Justification Is Retrospective Rationalization

Watch what happens when someone asks "why did we decide to use this technology?"

The actual reason: the lead engineer had used it before and liked it. The documented justification: "After careful evaluation of alternatives against criteria X, Y, Z..."

The justification narrative is manufactured to satisfy auditors and future archaeologists of decision-making. It has almost no relationship to why decisions were actually made.

Worse: the people who can actually *do* the work often cannot articulate why their approach works. The expert diagnostician will give you a story about their reasoning, but the story is not the knowledge—it's a reconstruction for your benefit.

### Tacit Knowledge Doesn't Fit

JTB applies to propositions—statements that can be true or false. But much organizational knowledge is not propositional:

- Knowing *how* to negotiate a difficult contract
- Knowing *when* a project is going off track
- Knowing *who* can actually get things done

These are capabilities, sensitivities, and relationships. They cannot be expressed as "S knows that P" without violence to what they actually are.

The expert machinist "knows" when the cut sounds wrong. What proposition do they believe? They just hear it. The knowledge is in the hearing.

---

## Why Justified True Belief Fails AI Systems

The JTB problem becomes acute when we consider AI systems doing knowledge work.

### The Belief Question Is Malformed

If we insist on JTB, we must ask: does an LLM *believe* its outputs? This question has no answer because it's built on a category error. LLMs are not the kind of system that has beliefs. The concept simply doesn't apply.

But the LLM can often *do the task*. If I ask it to write a function that sorts a list, it produces working code. If I ask it to summarize a document, it produces an accurate summary.

Under JTB, this is impossible. Knowledge requires belief. The AI has no beliefs. Therefore the AI has no knowledge. Therefore it can't do knowledge work. And yet—it does. The framework has produced a contradiction with observable reality.

### Capability Without Mental States

AI systems have no beliefs to be justified, no truth-tracking mental states, no justification chains they construct or evaluate. What they have is the ability to produce outputs that accomplish tasks.

If knowledge management is about JTB, then AI systems are permanently outside its scope—even as they increasingly *do* knowledge work. This is a framework failure, not an AI limitation.

### The Coherence of Agent-Relative Tacitness

Here's a key insight: knowledge that is tacit for one agent may be explicit for another.

For a human doctor: "This patient presentation suggests condition X" may be tacit—she can't fully articulate why.

For an AI trained on millions of cases: The same diagnostic conclusion may be entirely computable from patterns in the training data.

Tacitness is not a property of knowledge itself but of the relationship between knowledge and knower. This only makes sense if knowledge is understood as capability. A capability is tacit when the agent can exercise it but cannot articulate how. This is agent-relative by definition.

Under JTB, tacitness is a puzzle (how can you believe something you can't state?). Under capability views, it's obvious (of course abilities can exceed articulation).

---

## The Operational Definition

Here is the alternative:

**An agent has knowledge K with respect to outcome O in context C if and only if the agent can reliably produce O in C.**

Unpack this:

**Agent**: Any system capable of action—human, AI, organization, human-AI hybrid. The substrate is irrelevant. The question is whether it can act.

**Outcome**: Observable, verifiable state of the world. Working code. Healed patient. Profitable trade. Valid proof. Not a belief state—a state of affairs.

**Context**: The conditions under which capability is claimed. Knowledge claims are always bounded. "Can diagnose appendicitis in routine presentations" is a different claim than "can diagnose any abdominal condition."

**Reliably**: Not once, not by luck, but consistently across instances. The reliability threshold varies by domain—surgery demands higher reliability than brainstorming—but the requirement for repeatability is universal.

This definition has several features that matter:

**It's operational.** You verify knowledge by observing performance, not by inspecting mental states.

**It's agent-relative.** The same capability may be present in one agent and absent in another, tacit for one and explicit for another.

**It's measurable.** Degrees of knowledge correspond to reliability of outcome production across context instances.

**It handles AI naturally.** If the system can do the task, it has the knowledge. Whether it "understands" is immaterial.

---

## The Practical Cost of Getting This Wrong

JTB isn't just philosophically incorrect. It actively damages knowledge management practice.

### Documentation Instead of Capability

If knowledge equals justified true belief, then knowledge management equals getting beliefs documented with their justifications.

This produces wikis full of procedures no one follows, best practice documents describing idealized versions of work, and knowledge bases that grow stale the moment they're published.

Meanwhile, actual capability lives in people who can do the work but can't fully document how, systems configured to perform but whose logic is opaque, relationships that enable coordination but can't be captured, and situated judgment that adapts to circumstances but resists generalization.

JTB points organizations at documentation artifacts while actual capability walks out the door every night.

### Capture Projects That Capture Nothing

Every organization has run a "knowledge capture" initiative before key people retired. Most fail.

They fail because they try to capture *propositions*—what the expert believes and why. But the expert's capability is not primarily propositional. It's pattern recognition built over years, relationships and trust with key people, judgment about when rules apply and when they don't, embodied skill that can't be verbalized.

You can't capture these by asking "what do you know?" You might transfer them through apprenticeship (joint practice) or by building systems that embody the capability. But propositional capture projects miss what matters.

### Credentials Instead of Outcomes

JTB encourages measuring knowledge by justification quality: can you articulate good reasons for your beliefs?

This manifests as credentialism. Degrees prove you know things. Certifications prove expertise. Documentation proves organizational knowledge.

But credentials correlate poorly with capability. The certified project manager may deliver projects late. The degreed engineer may write unmaintainable code. The documented process may not work.

Outcome-based assessment asks different questions: Can you do the task? Do your interventions work? Do people and systems under your influence succeed?

---

## What This Foundation Makes Possible

### Executable Knowledge Architecture

EKA asks AI to translate expert intent into executable code rather than to provide answers directly. This only makes sense under a capability definition of knowledge.

The expert has knowledge: the capability to recognize correct outcomes, specify intent, and verify alignment. The AI has different knowledge: the capability to produce executable implementations from specifications.

Neither alone constitutes the knowledge needed to solve problems reliably. Together, through a verification loop, they produce validated solutions. The composite system has knowledge that neither component has alone.

Under JTB, this is incoherent—who "believes" the solution? Under capability definitions, it's precise—the human-AI system can reliably produce verified solutions.

### Agent-Relative Tacitness

Tacit knowledge has always been a puzzle for KM. If knowledge is justified true belief, what does it mean for a belief to be "tacit"? How can you believe something you can't state?

Under capability definitions, tacitness is straightforward: knowledge is tacit for agent B when no representational path exists from the knower to B. The capability exists but cannot be transferred through any available encoding and decoding process.

This makes tacitness agent-relative rather than intrinsic. The same knowledge can be tacit between a machinist and a novice, explicit between the machinist and another machinist, and newly explicit between the machinist and an AI system that can decode the machinist's demonstrations.

### Tacit Space Shrinkage

As AI capabilities expand, more knowledge becomes transferable that was previously locked in human expertise. An AI that can learn from video demonstrations can decode knowledge that was previously tacit because the only transfer path was embodied apprenticeship.

This "shrinkage of tacit space" is only coherent if tacitness is about transfer relationships, not intrinsic properties—which is only coherent if knowledge is capability, not belief.

---

## The Lineage

This isn't a novel position invented to make AI convenient. It draws on established traditions:

**Pragmatic philosophy** (Peirce, James, Dewey): Knowledge defined by practical consequences and action. Dewey's instrumentalism treats ideas as tools for problem-solving, not representations of fixed truths. "Knowledge as an instrument of action."

**Procedural knowledge** (Ryle, Polanyi): "Knowing how" cannot be reduced to "knowing that." We know more than we can tell. Knowledge is about being able, not about holding propositions.

**Organizational capability** (Senge, Sveiby, Teece): "Knowledge is the capacity for effective action. There is no capacity for effective action in a database." Dynamic capabilities as the firm's ability to produce outcomes in changing conditions.

**Cybernetics** (Ashby, Beer): Knowledge as variety management for effective regulation. A control system must be capable of producing the responses required to manage disturbances—capability, not content.

The capability view isn't a convenient fiction. It's the natural framework for anyone focused on what organizations can actually do.

---

## For Practitioners

Stop asking "what do we know?" Start asking "what can we do?"

Stop trying to capture knowledge as propositions. Start focusing on capability transfer through practice, pairing, and system-building.

Stop measuring knowledge by documentation. Start measuring by outcomes, task completion, and capability demonstration.

Stop treating AI as outside knowledge systems. Start treating AI capabilities as part of organizational capability, regardless of whether they "believe" anything.

Stop requiring justification chains. Start accepting that working capability is knowledge, whether or not the agent can explain it.

---

## The Foundation

This definition—knowledge as capability to produce outcomes—is the foundation on which everything else rests.

Agent-relative tacitness only makes sense if knowledge is capability (because tacitness becomes about transfer of capability, not about hidden beliefs).

Tacit space shrinkage only makes sense if tacitness is agent-relative (because AI capabilities can open new transfer paths).

Executable Knowledge Architecture only makes sense if human-AI systems can have composite knowledge (because capability can be distributed across agents).

The governance challenges I've written about—knowledge that is explicit-for-systems but opaque-for-humans—only make sense if explicitness is about transfer relationships, not about propositional content.

Without this foundation, you're trying to build on epistemological quicksand. With it, the frameworks become precise, measurable, and actionable.

We're in the business of doing things. The definition of knowledge should reflect that.
