---
title: "Knowledge as Capability: An Operational Foundation for Human-AI Cooperation"
date: 2024-11-12
description: "Defining knowledge as capability to produce outcomes, the foundational claim that makes human-AI cooperation coherent."
originalPublication: "Originally published November 2024"
type: foundational
tags: ["Epistemic Rigor"]

# Knowledge graph metadata
graph_id: knowledge-capability
defines: ["Knowledge as Capability", "Human-AI Composite Knowledge"]
themes: ["Epistemic Rigor"]
---

> *This essay provides the foundational definition on which several other pieces in this collection depend. Cross-references reflect the current state of an evolving framework. The version here is authoritative.*

All of my writing rests on a foundational equivalence: for organizations, **knowledge is the capability to reliably produce outcomes**. I use "knowledge" and "capability to produce outcomes" interchangeably. This essay is in support of that equivalency.

This is a practical foundation for governing human-AI cooperation, not a theory about what knowledge "really is" in some metaphysical sense. Organizations need to work with AI systems. The philosophical question of whether those systems have mental states is, for most practical purposes, unanswerable and operationally secondary.

For nearly a century, machines have been producing outcomes we'd call "knowledge work" if produced by humans. The Enigma machine embodied cryptographic knowledge in its rotor wiring. A TI-30 calculator embeds trigonometric knowledge you don't need to possess to use. Expert systems of the 1980s were explicitly called "knowledge-based systems." None of this was controversial.

The change is in capability, not kind. Large language models produce working code, accurate summaries, and valid analyses across domains far broader than any previous system. The question of whether they "really know" things has become urgent precisely because their outputs are useful enough to matter.

Everything else I've written about structuring human-AI knowledge work rests on this foundation. Without it, AI capabilities sit awkwardly outside existing frameworks. With it, human and AI contributions become commensurable.

---

## The Practical Problem

Epistemology is the branch of philosophy concerned with knowledge: what it is, how we acquire it, when we can legitimately claim to have it. Traditional epistemology, up to the 1960s at least, largely settled on a working definition: knowledge is *justified true belief*.

The three conditions are independent and all must hold. You cannot have knowledge that the earth is flat. It fails the truth condition regardless of how sincerely you believe it or how convinced you are of your reasons. Belief that the earth is an oblate spheroid because a turtle told you also isn't knowledge. It happens to be true, but "a turtle told me" isn't justification. Correctly guessing tomorrow's lottery numbers isn't knowledge either but a lucky guess.

That's the epistemology of the 1960s. The debate over what constitutes "knowledge" has not gotten simpler or clearer since. But what matters for our purposes: all these frameworks share an assumption. They assume agents with mental states. Whether you require justification, reliable belief-forming processes, intellectual virtues, or sensitivity to truth, you're still asking about the epistemic status of a *believer*.

When an AI system produces working code, the traditional framework has no place to put that capability. Does the AI "believe" its outputs? The question has no answer because it's built on a category error. AI systems are not the kind of entity belief-language was designed to describe.

Yet AI demonstrably does useful work that, in humans, we'd call knowledge work. We face a choice: either insist on definitions that exclude AI from the category entirely, or find operational definitions that make human-AI cooperation coherent. I'm proposing the latter.

---

## What We Don't Know About Knowing

Before claiming that humans "have" knowledge while AI systems "cannot," we should acknowledge what we don't know.

Philosophy hasn't solved the solipsism problem. I cannot verify that you have experiences, only that you behave as if you do. Neuroscience hasn't solved the hard problem of consciousness. Psychology doesn't have a settled account of what cognition is or how it produces understanding. The confident claim that humans possess knowledge in some robust sense while AI systems categorically cannot assumes we know what knowledge-constituting mental states are and how to detect them. We don't. That knowledge is not established in any discipline.

We extend epistemic charity across the solipsism gap for other humans. I assume you have experiences because you behave as if you do, even though I cannot verify this. Reasonable, but a choice, not a proof. The same logic applies to AI systems. We don't know how they achieve their results. We can't rule out that something analogous to mental states emerges from their operation. Confidently denying AI mental states requires the same leap of faith as confidently affirming human mental states. We just happen to be more comfortable with one leap than the other.

I'm not claiming AI systems have consciousness or experiences. I'm claiming we lack the theoretical foundations to make confident pronouncements either way. The dismissal is as unjustified as the affirmation would be. Some people dismiss AI capabilities definitionally: "it's just pattern matching, it doesn't really understand." But this assumes a clarity about human understanding that doesn't exist.

"It's just matrix multiplication" has exactly the same logical force as "it's just electrochemistry." Which is to say, none. The fact that we can describe a process in mechanical terms tells us nothing about whether that process constitutes understanding. We can describe human brains in mechanical terms too. The people making these dismissive arguments are treating "understanding" as a settled concept with known criteria, then confidently pronouncing that AI fails to meet them. But we don't have those criteria. No discipline does.

The honest position is epistemic humility about mechanisms combined with pragmatic attention to outcomes. We should judge by what systems can do, not by confident claims about what they can or cannot "really" be.

---

## Embedded Knowledge Has a Long History

The idea that artifacts can embody knowledge is not new or controversial. It has a seventy-year intellectual lineage.

**Cybernetics** established the framework. W. Ross Ashby's Law of Requisite Variety (1956) states that effective control requires a regulator as complex as the system being regulated. Conant and Ashby's Good Regulator Theorem (1970) makes this explicit: "Every good regulator of a system must be a model of that system."

The epistemological implication is direct. A thermostat must embody knowledge about temperature dynamics to regulate them. The knowledge is in the mechanism, not in any mental state. David Armstrong used thermometer readings as a model for externalist epistemology precisely because they track temperature without believing anything.

**Expert systems** in the 1980s were explicitly called "knowledge-based systems." Edward Feigenbaum defined them as programs that use "knowledge and inference procedures to solve problems that are difficult enough to require significant human expertise." DENDRAL, MYCIN, and their successors contained "knowledge bases." The terminology was deliberate. The philosophical debate was about whether this knowledge was *adequate*, not whether calling it knowledge was appropriate.

**Distributed cognition** (Edwin Hutchins) showed that cognitive processes extend across human-artifact systems. Navigation on a ship isn't done by any single mind but by a system including people, charts, compasses, and procedures. The knowledge is distributed across the network.

**The Extended Mind thesis** (Clark and Chalmers, 1998) argues that cognitive processes can extend beyond the brain. Their Parity Principle: if a process in the world functions identically to one we'd recognize as cognitive if it happened in the head, it's part of the cognitive system. A notebook can be external memory in the same sense that biological memory is internal memory.

This isn't a novel position I invented to accommodate AI. It's the natural framework for anyone focused on what systems can actually do.

---

## The Operational Definition

The operational move is simple: we're equating "knowing how" with "being able to do reliably." Where traditional epistemology asks "does the agent have justified true belief?", we ask "can the agent produce the outcome?" From here on, "can" carries the full weight that "knows" carries in traditional frameworks.

The key terms:

**Agent**: Any system capable of action. Human, AI, organization, human-AI hybrid. The underlying implementation is irrelevant. What matters is whether it can act.

**Outcome**: Observable, verifiable state of the world. Working code. Healed patient. Profitable trade. Valid proof. Not a belief state. A state of affairs.

**Context**: The conditions under which capability is claimed. Knowledge claims are always bounded. "Can diagnose appendicitis in routine presentations" is a different claim than "can diagnose any abdominal condition."

**Reliably**: Not once, not by luck, but consistently across instances. The reliability threshold varies by domain. Surgery demands higher reliability than brainstorming. But the requirement for repeatability is universal.

This definition has several features:

**It's operational.** You verify knowledge by observing performance, not by inspecting mental states.

**It's agent-relative.** The same capability may be present in one agent and absent in another.

**It's measurable.** Degrees of knowledge correspond to reliability of outcome production across context instances.

**It handles AI naturally.** If the system can do the task, it has the knowledge. Whether it "understands" in some deeper sense is a separate question we can't currently answer.

---

## What This Gets Us

### Tacitness Becomes Relational

Traditional discussions of tacit knowledge (Polanyi's "we know more than we can tell") treat tacitness as an intrinsic property of knowledge itself. Under an operational definition, tacitness becomes a relationship between knowledge and knower.

Knowledge is tacit for agent B when no representational path exists from the knower to B. The capability exists but cannot be transferred through any available encoding and decoding process.

This makes tacitness agent-relative rather than intrinsic. The same knowledge can be tacit between a machinist and a novice, explicit between the machinist and another machinist, and newly explicit between the machinist and an AI system that can decode the machinist's demonstrations.

For a human doctor: "This patient presentation suggests condition X" may be tacit. She can't fully articulate why.

For an AI trained on millions of cases: The same diagnostic conclusion may be entirely computable from patterns in the training data.

Under belief-based frameworks, this insight is hard to express. Under capability definitions, it becomes natural. And it opens new possibilities for human-AI collaboration in knowledge transfer.

### Composite Knowledge Becomes Coherent

[Executable Knowledge Architecture](/writing/executable-knowledge-architecture/) implements [assistant pattern](/writing/ai-oracle-vs-assistant/)—asking AI to execute rather than answer—for professional work: it asks AI to translate expert intent into executable code rather than to provide answers directly. This only makes sense under a capability definition.

The expert has knowledge: the capability to recognize correct outcomes, specify intent, and verify alignment. The AI has different knowledge: the capability to produce executable implementations from specifications.

Neither alone constitutes the knowledge needed to solve problems reliably. Together, through a verification loop, they produce validated solutions. The composite system has capability that neither component has alone.

Under belief-based definitions, this is incoherent. Who "believes" the solution? Under capability definitions, it's precise. The human-AI system can reliably produce verified solutions.

### Tacit Space Can Shrink

As AI capabilities expand, more knowledge becomes transferable that was previously locked in human expertise. An AI that can learn from video demonstrations can decode knowledge that was previously tacit because the only transfer path was embodied apprenticeship.

This "shrinkage of tacit space" is only coherent if tacitness is about transfer relationships, not intrinsic properties. And that's only coherent if knowledge is capability, not belief.

---

## What This Doesn't Claim

I'm not claiming AI systems have consciousness, phenomenal experience, or understanding in whatever sense those terms properly apply to humans. Those questions remain open.

I'm not claiming the operational definition captures everything philosophers mean by "knowledge." It doesn't. It deliberately brackets questions about justification, mental states, and epistemic normativity that traditional epistemology addresses.

I'm not claiming to extend or improve knowledge management theory as a discipline. The major KM theorists built frameworks primarily for human knowers with consciousness, embodiment, and social relationships. Some of their definitions, like Senge's "capacity for effective action," actually support the thesis here when read carefully. Others, like Nonaka's emphasis on tacit knowledge as inseparable from human subjectivity, don't. I'm engaging with these traditions where they're useful, not claiming wholesale endorsement.

What I'm claiming is narrower: for the practical purpose of governing human-AI cooperation, capability-based definitions work better than belief-based definitions. They let us ask the questions that matter operationally: Can the system do the task? How reliably? In what contexts? What are the failure modes?

---

## The Intellectual Lineage

This operational approach draws on established traditions, even if those traditions don't fully support all implications:

**Pragmatic philosophy** (Peirce, James, Dewey): Knowledge defined by practical consequences and action. Dewey's instrumentalism treats ideas as tools for problem-solving, not representations of fixed truths. "Knowledge is an instrument of action." Dewey's naturalistic continuity, treating human cognition as continuous with biological adaptation, provides the closest classical support for extending knowledge vocabulary to non-human systems.

**Procedural knowledge** (Ryle, Polanyi): "Knowing how" cannot be reduced to "knowing that." We know more than we can tell. Knowledge is about being able, not about holding propositions. Polanyi's framework was built for embodied human knowers, but his emphasis on capability over articulation points in a useful direction.

**Cybernetics** (Ashby, Beer): Knowledge as variety management for effective regulation. A control system must be capable of producing the responses required to manage disturbances. The Good Regulator Theorem directly implies that effective regulators embody models of what they regulate.

**Reliabilism** in epistemology (Goldman, Armstrong): Knowledge as true belief produced by reliable processes. This externalist approach doesn't require conscious access to justification. What matters is the reliability of the process, not the agent's introspective awareness of that reliability.

**Organizational capability** (Senge): Peter Senge defined knowledge as "the capacity for effective action," then added: "There is no capacity for effective action in a database." This second clause is often read as excluding machines from knowledge. It shouldn't be. A database is passive storage: it retrieves on query but doesn't *do* anything. A calculator is categorically different. It doesn't look up sin(45°) in a table; it computes the result using algorithms encoded in its processor. That's embedded effective action capability, not retrieval. The same applies to language models: they generate outputs through weighted transformations, not database lookups. Senge's exclusion of databases is correct. His definition, knowledge as capacity for effective action, actually supports the thesis that computational systems can embody knowledge.

I'm not claiming all these thinkers would endorse my specific proposal. Some would reject it. But the intellectual resources for capability-based approaches to knowledge exist and have been developed over decades.

---

## For Practitioners

The capability definition adds questions to your toolkit:

**Alongside "what do we know?"** ask "what can we do?" The former maps your documented knowledge; the latter maps your actual capabilities. Both matter.

**Alongside propositional capture**, use practice, pairing, and system-building. Documentation works for some knowledge; other methods work for others. The capability lens helps you choose.

**Alongside documentation metrics**, track outcomes. Completion rates, success rates, capability demonstrations. The goal is capability transfer; measure whether it's happening.

**Include AI in your capability assessments.** AI capabilities are organizational capabilities. They can receive knowledge transfer (training, prompting) and contribute to knowledge work. The capability definition gives you vocabulary for this.

**Accept that capability can exceed articulation.** Experts who can do but cannot fully explain still have knowledge worth transferring. Work with them to find transfer paths, including AI-assisted articulation.

---

## The Foundation

This definition, knowledge as capability to produce outcomes, is the foundation on which the rest of my framework rests.

Agent-relative tacitness only makes sense if knowledge is capability, because tacitness becomes about transfer of capability, not about hidden beliefs.

Tacit space shrinkage only makes sense if tacitness is agent-relative, because AI capabilities can open new transfer paths.

Executable Knowledge Architecture only makes sense if human-AI systems can have composite capability, distributed across agents.

The governance challenges I've written about, knowledge that is explicit-for-systems but opaque-for-humans, only make sense if explicitness is about transfer relationships, not about propositional content.

This isn't a replacement for epistemology or knowledge management. Those disciplines move at academic pace. Organizations deploying AI systems (by which I mean all organizations that want a future that is not Amish hand-crafted furniture building) don't have that luxury.

If it can do the job reliably, it has the knowledge. That's the whole claim.
